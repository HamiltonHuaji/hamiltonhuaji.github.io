---
title: Notes on Intepretability
date: 2023-05-21 16:37:40
mathjax: true
tags:
- Large Language Models
---

最近 LLMs (大语言模型)火得<del>一比吊糟</del>, 笔者也不能免俗, 因此弃坑光影包(并没有, 其实还在写)转战此处.

相比于一些连线性代数都没学好而每天阅读 transformer 基础教程的热衷于制造 fancy 字符串拼接轮子的前端程序员和勤于转发 10 个你必须知道的 prompt 技巧并喜欢制造 LLM 蜈蚣的产品经理[^1], 笔者无意于在谈天狗屁通本体之外糊一层技术含量稀薄无比的壳. 模型参数量数据量大推理能力就是强, 上下文窗口大就是能碾压基于段落向量嵌入存取上下文的方法, 这是无论如何写 prompt 都改变不了的对比关系. 在价值链上, prompt 必将成为 CRUD 类似物, 也许可以养活一些 prompt boy, 但也可以一夜之间让他们被开掉; 一切不碰 LLM 权重的魔改套壳都不能从根本上提高 LLM 的能力.

训练 LLaMa 产生了大约 $10^3$ 吨二氧化碳. 如此多的二氧化碳就已经是笔者买不起的了, 因此我等穷人似乎也只能去做 prompt boy. 一种不做 prompt boy 的可能的路径是转而像神经科学一样去解释 LLM 输出. 尽管关于可解释性的研究不能直接产出更强大或者更快的 LLM, 但说不定可以给出一些关于 LLM 的结构/初始化/训练过程/语料库构成如何影响 LLM 能力的 insight, 来提示我们如何改进 LLM 的能力, 或是在不损害能力的情况下排放更少的二氧化碳.

<!-- more -->

以上愿景似乎还不足以让可解释性研究被单列出来作为一个领域(而只作为 LLM 训练者的业余休闲). 感谢终结者系列, 感谢黑客帝国系列, 感谢所有热衷于以人工智能叛乱人类灭绝为题材的电影小说作者描摹了充满心机能力强大的人工智能形象, 公众对于人工智能危险性的关注似乎与它的实际能力不太相称; 感谢互联网上所有的喷子, 语言模型的友好性的价值被大大凸显; 感谢所有以 prompt 为核心价值的产品和功能, 其产品经理试图建立壁垒的努力和乐子人偷 prompt 或白嫖 token 量的努力让反越狱变得看起来的确是一个重要的安全措施. 所有这一切对人工智能的可靠性/友好性/忠诚性的期望带来了对理解 LLM 的幻觉的需求[^2], 从而也养活了不少以对齐(Alignment)为主要拉赞助理由的可解释性研究人员. 例如, 如果我们需要

+ 判断 LLM 是否在进行欺骗
+ 判断 LLM 是否确实学到了我们在预训练期间提供的知识
+ 纠正 LLM 中存储的错误知识
+ 检查 LLM **真正**[^3]的推理过程
+ 保证 LLM 能在复杂环境下进行正确的指代消解
+ 阻止 LLM 将"金贵"的 prompt 傻乎乎地输出给用户

我们就仍然必须深入 LLM, 去解释这个神秘的黑盒子里究竟发生了啥. 对这个黑盒子可以有好多层面的理解:

1. 检查黑盒子里每个神经元的激活条件, 并尝试总结出每个神经元各自的激活规律.
2. 将黑盒子内紧密协作的权重看作一条线路(circuit), 对每条线路的工作方式进行概括, 并对于一些能完成一定复杂度的任务的组合线路, 分解出各线路在其中发挥的作用.
3. 让黑盒子去处理一个因果图已知的问题, 并搜索黑盒子的内在表征, 以把这些表征与因果图中的中间变量对应起来.

更间接一些的理解有:

4. 检查 LLM 内常见的子结构, 并通过消融实验来证明这些子结构在训练中的出现与 LLM 的 ICL(In-Context Learning) 能力有关.
5. 通过总结神经元激活条件与层数的关联, 得出低层神经元储存词法句法等知识, 高层神经元储存语义和事实信息的结论.

笔者比较好(2)这一口, 因此本文将先介绍这一层面的理解所需基本的框架. 本文剩余部分是笔者阅读可解释性相关论文的笔记. 为了阅读本文, 读者应具有对基于 transformer 的语言模型的基本知识.

## [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html#pointer-arithmetic)

这一数学框架来自 transformer-circuits.pub.

首先回顾 pre-norm transformer LLM 的基本结构. transformer 处理的是 token 序列. 一个 token A 通过取出嵌入矩阵 $W_E$ 的某一行得到它的向量表示 $x_A$. 这一表示的维度一般记为 `d_model`, 大小往往成百上千. 随后 transformer 通过堆叠 $l$ 层基本的构建块来逐步处理向量表示, 得到最终的表示 $x_{Al}$. 在这之后, $x_{Al}$被送入反嵌入矩阵$W_{U}$, 得到所预测的词的对数概率(logits). 最终, 可以通过 beam search 等方法求得具体预测所得的 token.

pre-norm transformer 的构建块具有以下形式:

$$\begin{align*}h_{Ai}&=x_{Ai}+\text{Attn}(x_{Bi}, \forall B\ \text{precedes}\ A)\\
x_{Ai+1}&=h_{Ai}+\text{FFN}(h_{Ai})\\
&=x_{Ai}+\text{Attn}(x_{Bi}, \forall B\ \text{precedes}\ A)+\text{FFN}(h_{Ai})
\end{align*}$$

其中, $x_{Ai}$以外的部分叫残差流, 它类似于计算机中的总线, 但信息的传递是从低层单向传到高层的. $\text{Attn},\text{FFN}$项则向残差流中写入或删除信息, 以供高层神经元读取或改变最终输出的值.

值得在 transformer 的语境下以一种直观而不严谨的方式定义信息(的类似物). 考虑网络中的任何一个位置上的向量. 该向量有表观维度$n$, 即实际用于表示它的浮点数的个数. 也会有内在维度$d$, 即将该向量视作由输入token决定的随机变量时, 最大的线性无关分量的个数. 内在维度在经过秩不低于$d$的线性变换时不损失信息, 而在经过更低秩的变换时, 内在维度也随之变低.


. 如果它在网络中被进行了一个秩不低于$d$的线性变换, 然后被送入别的什么地方, 那么这个变换就是信息无损的, 因为接收者(往往是一个$\text{Attn}$项或$\text{FFN}$项)总是对输入自带一个线性变换的"入口"参数, 可以构造出$d$本身或者由$d$做任意线性变换的结果. 反之, 如果该向量被进行了一个秩低于$d$的线性变换, 那么就一定有信息在这个过程中损失了. 如果该向量从$d$维开始, 被进行了一个升维变换, 则它的信息量不变; 如果随后又进行了一个非线性变换, 那么它的

![跨token的信息流动](residual-stream-attention.png)



![关于单个token的残差流](residual-stream.png)

而在残差流中传输的$x_l$是一个高维向量, 其维度由网络超参数决定. 相比之下, 每个 token 对应的词向量嵌入空间并不需要占据这么高的维度. 因此, 对于某个 token, 第$l$层输出的值$x_l$可以概念性地写为如下形式:

$$x_l=R \cdot P \cdot \text{concat}(w_e, p_e, u_a, u_b, \dots)$$

其中, $w_e$ 表示该词原本的嵌入向量, $p_e$ 表示位置嵌入向量, $u_a$, $u_b$ 等是之前的层向残差流中写入的一些额外信息. 这些向量表达了独立的信息, 因此处于互不相干的空间中. 通过对它们直接连接的结果施加一个投影操作 $P$ 和旋转操作 $R$, 就得到了在网络中的层间实际传输的信息的表示.


Attention 机制则为信息在不同 token 之间的流动提供了手段. 例如, 在第 $l$ 层上, token B 上的一个注意力头可能会对 token A 产生高的注意力, 并取得 A 的 v 向量 $v_{Al}$. 这一向量等同于提取了 A 的

// TODO

[^1]: 是的,这确实是不客观的充满偏见的地图炮,但地图炮很爽;这一段是照着我的推特timeline写的,你也不希望自己的推特timeline被这种东西充满,不是吗?
[^2]: 也许有人会说,理解LLM中每一个注意力头的作用和每个嵌入维度的作用就足以理解LLM了.然而,LLM终究是一个足够复杂的系统,由于superposition的存在,很难说能保证不遗漏在复杂上下文中嵌入维度的特殊语义;组合起来的注意力头会产生远比单个注意力头复杂的行为.在这种情况下,也许对LLM的理解只能停留在一种幻觉的层面(就如同笔者尽管熟知计算机体系结构,却仍然会被主板的硬件bug折磨一样).
[^3]: 通过CoT等技巧诱导LLM输出的推理过程不见得是其真正使用的推理过程.这又是一种自以为理解LLM的幻觉,即把它们看作诚实的人类,误以为它们会准确地说出自己的内心戏.
